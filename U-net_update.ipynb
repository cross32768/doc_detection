{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 256\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 768\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_x0.8_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "conducted_experiment_name_list = sorted(os.listdir(result_dir))\n",
    "new_experiment_name = 'experiment_%03d' % (int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version : {}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version : {}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize : {}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size : {}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size : {}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset : {}\\n'.format(image_dir))\n",
    "    f.write('used label dataset : {}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data : {}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data : {}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = np.random.randint(0, h - th)\n",
    "        j = np.random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = torchvision.transforms.functional.crop(img1, i, j, h, w)\n",
    "        img2_cropped = torchvision.transforms.functional.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping))\n",
    "tf_image_train = transforms.ToTensor()\n",
    "tf_label_train = transforms.ToTensor()\n",
    "tf_image_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping,\n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "tf_label_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping, \n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image_train, tf_label_train)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                None, tf_image_validation, tf_label_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train : {}\\n'.format(tf_sync_train))\n",
    "    f.write('transform image for train : {}\\n'.format(tf_image_train))\n",
    "    f.write('transform label for train : {}\\n\\n'.format(tf_label_train))\n",
    "    f.write('transform image for validation : {}\\n'.format(tf_image_validation))\n",
    "    f.write('transform label for validation : {}\\n'.format(tf_label_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        max_channels = 1024\n",
    "        \n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_base_channels, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels*2, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels) + min(n_channels//2, max_channels)\n",
    "                n_out_channels = min(n_channels//2, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Conv2d(n_channels, 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 145320641\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (6): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(1536, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (6): UpSample(\n",
      "      (tc): ConvTranspose2d(192, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=7, n_base_channels=64)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters : {}\\n'.format(num_trainable_params))\n",
    "    f.write('Model : \\n{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer : \\n{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss : {}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 5\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[ 1/100] train_loss:0.6109 validation_loss:0.6983\n",
      "epoch[ 2/100] train_loss:0.5074 validation_loss:0.6388\n",
      "epoch[ 3/100] train_loss:0.4689 validation_loss:0.5610\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa29339bb38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(validation_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ64, 学習率5e-4というのが1つのうまく行く組合せらしい（他にもあると思うが、まだ見つけられていない）\n",
    "# 過学習を起こしている様子はあまりないが、validation lossの安定感に非常に欠ける\n",
    "# フィルタサイズは大きい方が性能が良い（当然といえば当然） 16とかだとマジで結果が悪い\n",
    "# 学習率のグラフで結果がたまに跳ね上がることがあるが、それは恐らく絵の画像に突き当たったりしているため"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
