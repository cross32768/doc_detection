{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 128\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 512\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_x0.8_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "# XXX:このプログラムはos.listdirがソートを保証していないことでうまく動かない可能性がある！\n",
    "conducted_experiment_name_list = os.listdir(result_dir)\n",
    "new_experiment_name = 'experiment_%03d' % (int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version : {}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version : {}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize : {}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size : {}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size : {}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset : {}\\n'.format(image_dir))\n",
    "    f.write('used label dataset : {}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2, random_state=0)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data : {}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data : {}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = np.random.randint(0, h - th)\n",
    "        j = np.random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = torchvision.transforms.functional.crop(img1, i, j, h, w)\n",
    "        img2_cropped = torchvision.transforms.functional.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping)) # use for training\n",
    "tf_sync_validation = RandomCropSync((height_for_validation_cropping, width_for_validation_cropping)) # use for validation\n",
    "tf_image = transforms.ToTensor() # use always\n",
    "tf_label = transforms.ToTensor() # use always\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image, tf_label)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                tf_sync_validation, tf_image, tf_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train : {}\\n'.format(tf_sync_train))\n",
    "    f.write('transform sync(image and label) for validation : {}\\n'.format(tf_sync_validation))\n",
    "    f.write('transform image for train and validation : {}\\n'.format(tf_image))\n",
    "    f.write('transform label for train and validation : {}\\n\\n'.format(tf_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                self.encoder.append(DownSample(n_channels, n_base_channels))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                self.encoder.append(DownSample(n_channels, n_channels*2))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                self.decoder.append(UpSample(n_channels, n_channels))\n",
    "            else:\n",
    "                self.decoder.append(UpSample(n_channels + n_channels//2, n_channels//2))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Conv2d(n_channels, 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 44725601\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): DownSample(\n",
      "      (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(1536, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(192, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): UpSample(\n",
      "      (tc): ConvTranspose2d(96, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=7, n_base_channels=32)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters : {}\\n'.format(num_trainable_params))\n",
    "    f.write('Model : \\n{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer : \\n{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss : {}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 5\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[ 1/100] train_loss:0.6403 validation_loss:0.6554\n",
      "epoch[ 2/100] train_loss:0.5708 validation_loss:0.6335\n",
      "epoch[ 3/100] train_loss:0.5413 validation_loss:0.6022\n",
      "epoch[ 4/100] train_loss:0.5224 validation_loss:0.5684\n",
      "epoch[ 5/100] train_loss:0.5067 validation_loss:0.5428\n",
      "epoch[ 6/100] train_loss:0.4964 validation_loss:0.5211\n",
      "epoch[ 7/100] train_loss:0.4851 validation_loss:0.4993\n",
      "epoch[ 8/100] train_loss:0.4791 validation_loss:0.4773\n",
      "epoch[ 9/100] train_loss:0.4697 validation_loss:0.4534\n",
      "epoch[10/100] train_loss:0.4615 validation_loss:0.4443\n",
      "epoch[11/100] train_loss:0.4543 validation_loss:0.4326\n",
      "epoch[12/100] train_loss:0.4448 validation_loss:0.4251\n",
      "epoch[13/100] train_loss:0.4413 validation_loss:0.4136\n",
      "epoch[14/100] train_loss:0.4329 validation_loss:0.4069\n",
      "epoch[15/100] train_loss:0.4252 validation_loss:0.4096\n",
      "epoch[16/100] train_loss:0.4164 validation_loss:0.3951\n",
      "epoch[17/100] train_loss:0.4106 validation_loss:0.3917\n",
      "epoch[18/100] train_loss:0.4044 validation_loss:0.3878\n",
      "epoch[19/100] train_loss:0.3974 validation_loss:0.3895\n",
      "epoch[20/100] train_loss:0.3939 validation_loss:0.3724\n",
      "epoch[21/100] train_loss:0.3883 validation_loss:0.3803\n",
      "epoch[22/100] train_loss:0.3794 validation_loss:0.3655\n",
      "epoch[23/100] train_loss:0.3758 validation_loss:0.3618\n",
      "epoch[24/100] train_loss:0.3696 validation_loss:0.3589\n",
      "epoch[25/100] train_loss:0.3634 validation_loss:0.3471\n",
      "epoch[26/100] train_loss:0.3582 validation_loss:0.3462\n",
      "epoch[27/100] train_loss:0.3535 validation_loss:0.3392\n",
      "epoch[28/100] train_loss:0.3468 validation_loss:0.3298\n",
      "epoch[29/100] train_loss:0.3420 validation_loss:0.3270\n",
      "epoch[30/100] train_loss:0.3390 validation_loss:0.3191\n",
      "epoch[31/100] train_loss:0.3313 validation_loss:0.3084\n",
      "epoch[32/100] train_loss:0.3293 validation_loss:0.3114\n",
      "epoch[33/100] train_loss:0.3219 validation_loss:0.3049\n",
      "epoch[34/100] train_loss:0.3172 validation_loss:0.3021\n",
      "epoch[35/100] train_loss:0.3122 validation_loss:0.3148\n",
      "epoch[36/100] train_loss:0.3080 validation_loss:0.2878\n",
      "epoch[37/100] train_loss:0.3042 validation_loss:0.2892\n",
      "epoch[38/100] train_loss:0.3004 validation_loss:0.2870\n",
      "epoch[39/100] train_loss:0.2933 validation_loss:0.2749\n",
      "epoch[40/100] train_loss:0.2885 validation_loss:0.2740\n",
      "epoch[41/100] train_loss:0.2855 validation_loss:0.2716\n",
      "epoch[42/100] train_loss:0.2827 validation_loss:0.2652\n",
      "epoch[43/100] train_loss:0.2783 validation_loss:0.2668\n",
      "epoch[44/100] train_loss:0.2736 validation_loss:0.2646\n",
      "epoch[45/100] train_loss:0.2720 validation_loss:0.2575\n",
      "epoch[46/100] train_loss:0.2674 validation_loss:0.2519\n",
      "epoch[47/100] train_loss:0.2681 validation_loss:0.2522\n",
      "epoch[48/100] train_loss:0.2601 validation_loss:0.2480\n",
      "epoch[49/100] train_loss:0.2581 validation_loss:0.2386\n",
      "epoch[50/100] train_loss:0.2544 validation_loss:0.2400\n",
      "epoch[51/100] train_loss:0.2527 validation_loss:0.2531\n",
      "epoch[52/100] train_loss:0.2532 validation_loss:0.2487\n",
      "epoch[53/100] train_loss:0.2429 validation_loss:0.2736\n",
      "epoch[54/100] train_loss:0.2428 validation_loss:0.2309\n",
      "epoch[55/100] train_loss:0.2388 validation_loss:0.2270\n",
      "epoch[56/100] train_loss:0.2352 validation_loss:0.2611\n",
      "epoch[57/100] train_loss:0.2369 validation_loss:0.4638\n",
      "epoch[58/100] train_loss:0.2296 validation_loss:0.3221\n",
      "epoch[59/100] train_loss:0.2267 validation_loss:0.2120\n",
      "epoch[60/100] train_loss:0.2241 validation_loss:0.2222\n",
      "epoch[61/100] train_loss:0.2236 validation_loss:0.2065\n",
      "epoch[62/100] train_loss:0.2188 validation_loss:0.1987\n",
      "epoch[63/100] train_loss:0.2164 validation_loss:0.1989\n",
      "epoch[64/100] train_loss:0.2105 validation_loss:0.2010\n",
      "epoch[65/100] train_loss:0.2157 validation_loss:0.1918\n",
      "epoch[66/100] train_loss:0.2106 validation_loss:0.1932\n",
      "epoch[67/100] train_loss:0.2065 validation_loss:0.1881\n",
      "epoch[68/100] train_loss:0.2029 validation_loss:0.1912\n",
      "epoch[69/100] train_loss:0.2018 validation_loss:0.1987\n",
      "epoch[70/100] train_loss:0.2022 validation_loss:0.1891\n",
      "epoch[71/100] train_loss:0.1970 validation_loss:0.1845\n",
      "epoch[72/100] train_loss:0.1983 validation_loss:0.1800\n",
      "epoch[73/100] train_loss:0.1935 validation_loss:0.1759\n",
      "epoch[74/100] train_loss:0.1913 validation_loss:0.1734\n",
      "epoch[75/100] train_loss:0.1855 validation_loss:0.2109\n",
      "epoch[76/100] train_loss:0.1868 validation_loss:0.1898\n",
      "epoch[77/100] train_loss:0.1846 validation_loss:0.1714\n",
      "epoch[78/100] train_loss:0.1799 validation_loss:0.1693\n",
      "epoch[79/100] train_loss:0.1790 validation_loss:0.1662\n",
      "epoch[80/100] train_loss:0.1788 validation_loss:0.1701\n",
      "epoch[81/100] train_loss:0.1784 validation_loss:0.1609\n",
      "epoch[82/100] train_loss:0.1794 validation_loss:0.1602\n",
      "epoch[83/100] train_loss:0.1763 validation_loss:0.1559\n",
      "epoch[84/100] train_loss:0.1766 validation_loss:0.1572\n",
      "epoch[85/100] train_loss:0.1714 validation_loss:0.1563\n",
      "epoch[86/100] train_loss:0.1693 validation_loss:0.1731\n",
      "epoch[87/100] train_loss:0.1662 validation_loss:0.1568\n",
      "epoch[88/100] train_loss:0.1668 validation_loss:0.1536\n",
      "epoch[89/100] train_loss:0.1639 validation_loss:0.1540\n",
      "epoch[90/100] train_loss:0.1679 validation_loss:0.1488\n",
      "epoch[91/100] train_loss:0.1643 validation_loss:0.1556\n",
      "epoch[92/100] train_loss:0.1645 validation_loss:0.1412\n",
      "epoch[93/100] train_loss:0.1590 validation_loss:0.1503\n",
      "epoch[94/100] train_loss:0.1617 validation_loss:0.1481\n",
      "epoch[95/100] train_loss:0.1613 validation_loss:0.1499\n",
      "epoch[96/100] train_loss:0.1599 validation_loss:0.1480\n",
      "epoch[97/100] train_loss:0.1559 validation_loss:0.1426\n",
      "epoch[98/100] train_loss:0.1515 validation_loss:0.1392\n",
      "epoch[99/100] train_loss:0.1579 validation_loss:0.1387\n",
      "epoch[100/100] train_loss:0.1593 validation_loss:0.1391\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9381292b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(validation_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ64, 学習率5e-4というのが1つのうまく行く組合せらしい（他にもあると思うが、まだ見つけられていない）\n",
    "# 過学習を起こしている様子はあまりないが、validation lossの安定感に非常に欠ける\n",
    "# フィルタサイズは大きい方が性能が良い（当然といえば当然） 16とかだとマジで結果が悪い\n",
    "# 学習率のグラフで結果がたまに跳ね上がることがあるが、それは恐らく絵の画像に突き当たったりしているため"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
