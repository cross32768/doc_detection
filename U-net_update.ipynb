{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 128\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 512\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_x0.8_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "conducted_experiment_name_list = sorted(os.listdir(result_dir))\n",
    "new_experiment_name = 'experiment_' + str(int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version:{}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version:{}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize:{}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size:{}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size:{}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset:{}\\n'.format(image_dir))\n",
    "    f.write('used label dataset:{}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2, random_state=0)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data:{}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data:{}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = F.crop(img1, i, j, h, w)\n",
    "        img2_cropped = F.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping)) # use for training\n",
    "tf_sync_validation = RandomCropSync((height_for_validation_cropping, width_for_validation_cropping)) # use for validation\n",
    "tf_image = transforms.ToTensor() # use always\n",
    "tf_label = transforms.ToTensor() # use always\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image, tf_label)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                tf_sync_validation, tf_image, tf_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train:{}\\n'.format(tf_sync_train))\n",
    "    f.write('transform sync(image and label) for validation:{}\\n'.format(tf_sync_validation))\n",
    "    f.write('transform image for train and validation:{}\\n'.format(tf_image))\n",
    "    f.write('transform label for train and validation:{}\\n\\n'.format(tf_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                self.encoder.append(DownSample(n_channels, n_base_channels))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                self.encoder.append(DownSample(n_channels, n_channels*2))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                self.decoder.append(UpSample(n_channels, n_channels))\n",
    "            else:\n",
    "                self.decoder.append(UpSample(n_channels + n_channels//2, n_channels//2))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Conv2d(n_channels, 3, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 11165091\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(192, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(96, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=5, n_base_channels=32)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters:{}\\n'.format(num_trainable_params))\n",
    "    f.write('Model:{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer:{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss:{}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 2\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
