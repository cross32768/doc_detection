{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 128\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 768\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_hot_x0.8_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "conducted_experiment_name_list = sorted(os.listdir(result_dir))\n",
    "new_experiment_name = 'experiment_%03d' % (int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version : {}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version : {}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize : {}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size : {}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size : {}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset : {}\\n'.format(image_dir))\n",
    "    f.write('used label dataset : {}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        label_name = label_name.replace('.jpg', '.png')\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data : {}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data : {}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = np.random.randint(0, h - th)\n",
    "        j = np.random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = torchvision.transforms.functional.crop(img1, i, j, h, w)\n",
    "        img2_cropped = torchvision.transforms.functional.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping))\n",
    "tf_image_train = transforms.ToTensor()\n",
    "tf_label_train = transforms.ToTensor()\n",
    "tf_image_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping,\n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "tf_label_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping, \n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image_train, tf_label_train)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                None, tf_image_validation, tf_label_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train : {}\\n'.format(tf_sync_train))\n",
    "    f.write('transform image for train : {}\\n'.format(tf_image_train))\n",
    "    f.write('transform label for train : {}\\n'.format(tf_label_train))\n",
    "    f.write('transform image for validation : {}\\n'.format(tf_image_validation))\n",
    "    f.write('transform label for validation : {}\\n\\n'.format(tf_label_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        max_channels = 1024\n",
    "        \n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_base_channels, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels, use_bn=False))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels*2, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels) + min(n_channels//2, max_channels)\n",
    "                n_out_channels = min(n_channels//2, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "                            nn.Conv2d(n_channels, n_channels//2, kernel_size=1, stride=1, padding=0),\n",
    "                            nn.BatchNorm2d(n_channels // 2),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv2d(n_channels//2, 3, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=255):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.nll_loss(F.log_softmax(inputs, dim=1), targets.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 195341699\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (5): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (6): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(1536, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (6): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " CrossEntropyLoss2d(\n",
      "  (nll_loss): NLLLoss2d()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaito/.local/lib/python3.5/site-packages/torch/nn/modules/loss.py:217: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
      "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n",
      "/home/kaito/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=7, n_base_channels=128)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "criterion = CrossEntropyLoss2d()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters : {}\\n'.format(num_trainable_params))\n",
    "    f.write('Model : \\n{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer : \\n{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss : {}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 5\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.91 GiB total capacity; 9.43 GiB already allocated; 200.12 MiB free; 393.76 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-f3acaf5055c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-2183587bab7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-1360d1adda75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mconcated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_encoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_depth_encoder\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-54148d70cd65>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_batchnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.91 GiB total capacity; 9.43 GiB already allocated; 200.12 MiB free; 393.76 MiB cached)"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01167ebfd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(validation_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64, 5e-41\n",
    "# validation loss\n",
    "#  16\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
