{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 128\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 768\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_hot_xw0.8_h0.5_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "conducted_experiment_name_list = sorted(os.listdir(result_dir))\n",
    "new_experiment_name = 'experiment_%03d' % (int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version : {}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version : {}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize : {}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size : {}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size : {}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset : {}\\n'.format(image_dir))\n",
    "    f.write('used label dataset : {}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        label_name = label_name.replace('.jpg', '.png')\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data : {}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data : {}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = np.random.randint(0, h - th)\n",
    "        j = np.random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = torchvision.transforms.functional.crop(img1, i, j, h, w)\n",
    "        img2_cropped = torchvision.transforms.functional.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping))\n",
    "tf_image_train = transforms.ToTensor()\n",
    "tf_label_train = transforms.ToTensor()\n",
    "tf_image_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping,\n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "tf_label_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping, \n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image_train, tf_label_train)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                None, tf_image_validation, tf_label_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train : {}\\n'.format(tf_sync_train))\n",
    "    f.write('transform image for train : {}\\n'.format(tf_image_train))\n",
    "    f.write('transform label for train : {}\\n'.format(tf_label_train))\n",
    "    f.write('transform image for validation : {}\\n'.format(tf_image_validation))\n",
    "    f.write('transform label for validation : {}\\n\\n'.format(tf_label_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        max_channels = 1024\n",
    "        \n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_base_channels, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels, use_bn=False))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels*2, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels) + min(n_channels//2, max_channels)\n",
    "                n_out_channels = min(n_channels//2, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "                            nn.Conv2d(n_channels, n_channels//2, kernel_size=1, stride=1, padding=0),\n",
    "                            nn.BatchNorm2d(n_channels // 2),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Conv2d(n_channels//2, 3, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=255):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss(weight, size_average)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.bce_loss(F.log_softmax(inputs, dim=1), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 195341699\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (5): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (6): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(1536, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (6): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=7, n_base_channels=128)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "# criterion = CrossEntropyLoss2d()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters : {}\\n'.format(num_trainable_params))\n",
    "    f.write('Model : \\n{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer : \\n{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss : {}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 5\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[ 1/500] train_loss:0.6802 validation_loss:0.7057\n",
      "epoch[ 2/500] train_loss:0.6225 validation_loss:0.8896\n",
      "epoch[ 3/500] train_loss:0.5949 validation_loss:0.6621\n",
      "epoch[ 4/500] train_loss:0.5760 validation_loss:1.1776\n",
      "epoch[ 5/500] train_loss:0.5587 validation_loss:0.7201\n",
      "epoch[ 6/500] train_loss:0.5446 validation_loss:0.5916\n",
      "epoch[ 7/500] train_loss:0.5316 validation_loss:0.6725\n",
      "epoch[ 8/500] train_loss:0.5198 validation_loss:0.5738\n",
      "epoch[ 9/500] train_loss:0.5077 validation_loss:0.5660\n",
      "epoch[10/500] train_loss:0.4940 validation_loss:0.5647\n",
      "epoch[11/500] train_loss:0.4807 validation_loss:0.5224\n",
      "epoch[12/500] train_loss:0.4688 validation_loss:0.5778\n",
      "epoch[13/500] train_loss:0.4551 validation_loss:0.4704\n",
      "epoch[14/500] train_loss:0.4434 validation_loss:0.4259\n",
      "epoch[15/500] train_loss:0.4309 validation_loss:0.4375\n",
      "epoch[16/500] train_loss:0.4187 validation_loss:0.4026\n",
      "epoch[17/500] train_loss:0.4050 validation_loss:0.3951\n",
      "epoch[18/500] train_loss:0.3943 validation_loss:0.3727\n",
      "epoch[19/500] train_loss:0.3820 validation_loss:0.3815\n",
      "epoch[20/500] train_loss:0.3715 validation_loss:0.3669\n",
      "epoch[21/500] train_loss:0.3594 validation_loss:0.3336\n",
      "epoch[22/500] train_loss:0.3488 validation_loss:0.3182\n",
      "epoch[23/500] train_loss:0.3378 validation_loss:0.3134\n",
      "epoch[24/500] train_loss:0.3271 validation_loss:0.3020\n",
      "epoch[25/500] train_loss:0.3160 validation_loss:0.2905\n",
      "epoch[26/500] train_loss:0.3060 validation_loss:0.2869\n",
      "epoch[27/500] train_loss:0.2974 validation_loss:0.2790\n",
      "epoch[28/500] train_loss:0.2885 validation_loss:0.2773\n",
      "epoch[29/500] train_loss:0.2766 validation_loss:0.2590\n",
      "epoch[30/500] train_loss:0.2701 validation_loss:0.2465\n",
      "epoch[31/500] train_loss:0.2597 validation_loss:0.2414\n",
      "epoch[32/500] train_loss:0.2525 validation_loss:0.2332\n",
      "epoch[33/500] train_loss:0.2453 validation_loss:0.2297\n",
      "epoch[34/500] train_loss:0.2380 validation_loss:0.2201\n",
      "epoch[35/500] train_loss:0.2317 validation_loss:0.2124\n",
      "epoch[36/500] train_loss:0.2212 validation_loss:0.2076\n",
      "epoch[37/500] train_loss:0.2147 validation_loss:0.1996\n",
      "epoch[38/500] train_loss:0.2096 validation_loss:0.1956\n",
      "epoch[39/500] train_loss:0.2050 validation_loss:0.1984\n",
      "epoch[40/500] train_loss:0.1978 validation_loss:0.1914\n",
      "epoch[41/500] train_loss:0.1939 validation_loss:0.1774\n",
      "epoch[42/500] train_loss:0.1860 validation_loss:0.1729\n",
      "epoch[43/500] train_loss:0.1807 validation_loss:0.1677\n",
      "epoch[44/500] train_loss:0.1765 validation_loss:0.1695\n",
      "epoch[45/500] train_loss:0.1712 validation_loss:0.1602\n",
      "epoch[46/500] train_loss:0.1667 validation_loss:0.1628\n",
      "epoch[47/500] train_loss:0.1619 validation_loss:0.1700\n",
      "epoch[48/500] train_loss:0.1581 validation_loss:0.1486\n",
      "epoch[49/500] train_loss:0.1552 validation_loss:0.1515\n",
      "epoch[50/500] train_loss:0.1488 validation_loss:0.1538\n",
      "epoch[51/500] train_loss:0.1443 validation_loss:0.1536\n",
      "epoch[52/500] train_loss:0.1433 validation_loss:0.1354\n",
      "epoch[53/500] train_loss:0.1377 validation_loss:0.1560\n",
      "epoch[54/500] train_loss:0.1366 validation_loss:0.1394\n",
      "epoch[55/500] train_loss:0.1331 validation_loss:0.1672\n",
      "epoch[56/500] train_loss:0.1289 validation_loss:0.1292\n",
      "epoch[57/500] train_loss:0.1260 validation_loss:0.1190\n",
      "epoch[58/500] train_loss:0.1232 validation_loss:0.1567\n",
      "epoch[59/500] train_loss:0.1217 validation_loss:0.1495\n",
      "epoch[60/500] train_loss:0.1192 validation_loss:0.1252\n",
      "epoch[61/500] train_loss:0.1147 validation_loss:0.1213\n",
      "epoch[62/500] train_loss:0.1128 validation_loss:0.1330\n",
      "epoch[63/500] train_loss:0.1110 validation_loss:0.1027\n",
      "epoch[64/500] train_loss:0.1095 validation_loss:0.0981\n",
      "epoch[65/500] train_loss:0.1046 validation_loss:0.1166\n",
      "epoch[66/500] train_loss:0.1045 validation_loss:0.1095\n",
      "epoch[67/500] train_loss:0.1045 validation_loss:0.1039\n",
      "epoch[68/500] train_loss:0.1011 validation_loss:0.1028\n",
      "epoch[69/500] train_loss:0.1026 validation_loss:0.0966\n",
      "epoch[70/500] train_loss:0.0963 validation_loss:0.1253\n",
      "epoch[71/500] train_loss:0.0951 validation_loss:0.1176\n",
      "epoch[72/500] train_loss:0.0927 validation_loss:0.1044\n",
      "epoch[73/500] train_loss:0.0925 validation_loss:0.1103\n",
      "epoch[74/500] train_loss:0.0973 validation_loss:0.0826\n",
      "epoch[75/500] train_loss:0.0946 validation_loss:0.1007\n",
      "epoch[76/500] train_loss:0.0908 validation_loss:0.1478\n",
      "epoch[77/500] train_loss:0.0875 validation_loss:0.0886\n",
      "epoch[78/500] train_loss:0.0849 validation_loss:0.0830\n",
      "epoch[79/500] train_loss:0.0850 validation_loss:0.1369\n",
      "epoch[80/500] train_loss:0.0832 validation_loss:0.0778\n",
      "epoch[81/500] train_loss:0.0825 validation_loss:0.1092\n",
      "epoch[82/500] train_loss:0.0832 validation_loss:0.0753\n",
      "epoch[83/500] train_loss:0.0782 validation_loss:0.1033\n",
      "epoch[84/500] train_loss:0.0839 validation_loss:0.1145\n",
      "epoch[85/500] train_loss:0.0765 validation_loss:0.0782\n",
      "epoch[86/500] train_loss:0.0778 validation_loss:0.0940\n",
      "epoch[87/500] train_loss:0.0763 validation_loss:0.0718\n",
      "epoch[88/500] train_loss:0.0756 validation_loss:0.0663\n",
      "epoch[89/500] train_loss:0.0763 validation_loss:0.0660\n",
      "epoch[90/500] train_loss:0.0739 validation_loss:0.1170\n",
      "epoch[91/500] train_loss:0.0744 validation_loss:0.0954\n",
      "epoch[92/500] train_loss:0.0701 validation_loss:0.0831\n",
      "epoch[93/500] train_loss:0.0732 validation_loss:0.0636\n",
      "epoch[94/500] train_loss:0.0742 validation_loss:0.0828\n",
      "epoch[95/500] train_loss:0.0696 validation_loss:0.0776\n",
      "epoch[96/500] train_loss:0.0706 validation_loss:0.0649\n",
      "epoch[97/500] train_loss:0.0687 validation_loss:0.0770\n",
      "epoch[98/500] train_loss:0.0671 validation_loss:0.0698\n",
      "epoch[99/500] train_loss:0.0664 validation_loss:0.0614\n",
      "epoch[100/500] train_loss:0.0681 validation_loss:0.0629\n",
      "epoch[101/500] train_loss:0.0656 validation_loss:0.0656\n",
      "epoch[102/500] train_loss:0.0648 validation_loss:0.0547\n",
      "epoch[103/500] train_loss:0.0643 validation_loss:0.1146\n",
      "epoch[104/500] train_loss:0.0625 validation_loss:0.0714\n",
      "epoch[105/500] train_loss:0.0664 validation_loss:0.0647\n",
      "epoch[106/500] train_loss:0.0614 validation_loss:0.1283\n",
      "epoch[107/500] train_loss:0.0612 validation_loss:0.0567\n",
      "epoch[108/500] train_loss:0.0617 validation_loss:0.0579\n",
      "epoch[109/500] train_loss:0.0616 validation_loss:0.0690\n",
      "epoch[110/500] train_loss:0.0584 validation_loss:0.0496\n",
      "epoch[111/500] train_loss:0.0607 validation_loss:0.0513\n",
      "epoch[112/500] train_loss:0.0589 validation_loss:0.0668\n",
      "epoch[113/500] train_loss:0.0576 validation_loss:0.1001\n",
      "epoch[114/500] train_loss:0.0582 validation_loss:0.1056\n",
      "epoch[115/500] train_loss:0.0596 validation_loss:0.0578\n",
      "epoch[116/500] train_loss:0.0575 validation_loss:0.1514\n",
      "epoch[117/500] train_loss:0.0581 validation_loss:0.1501\n",
      "epoch[118/500] train_loss:0.0586 validation_loss:0.0775\n",
      "epoch[119/500] train_loss:0.0595 validation_loss:0.0718\n",
      "epoch[120/500] train_loss:0.0570 validation_loss:0.0503\n",
      "epoch[121/500] train_loss:0.0533 validation_loss:0.0746\n",
      "epoch[122/500] train_loss:0.0552 validation_loss:0.0446\n",
      "epoch[123/500] train_loss:0.0586 validation_loss:0.0499\n",
      "epoch[124/500] train_loss:0.0550 validation_loss:0.0511\n",
      "epoch[125/500] train_loss:0.0535 validation_loss:0.0503\n",
      "epoch[126/500] train_loss:0.0579 validation_loss:0.0542\n",
      "epoch[127/500] train_loss:0.0531 validation_loss:0.1084\n",
      "epoch[128/500] train_loss:0.0528 validation_loss:0.0446\n",
      "epoch[129/500] train_loss:0.0537 validation_loss:0.0453\n",
      "epoch[130/500] train_loss:0.0542 validation_loss:0.0501\n",
      "epoch[131/500] train_loss:0.0541 validation_loss:0.0466\n",
      "epoch[132/500] train_loss:0.0510 validation_loss:0.0635\n",
      "epoch[133/500] train_loss:0.0539 validation_loss:0.0873\n",
      "epoch[134/500] train_loss:0.0535 validation_loss:0.0485\n",
      "epoch[135/500] train_loss:0.0519 validation_loss:0.0452\n",
      "epoch[136/500] train_loss:0.0518 validation_loss:0.0670\n",
      "epoch[137/500] train_loss:0.0534 validation_loss:0.0465\n",
      "epoch[138/500] train_loss:0.0509 validation_loss:0.0481\n",
      "epoch[139/500] train_loss:0.0514 validation_loss:0.0536\n",
      "epoch[140/500] train_loss:0.0486 validation_loss:0.0436\n",
      "epoch[141/500] train_loss:0.0496 validation_loss:0.0514\n",
      "epoch[142/500] train_loss:0.0505 validation_loss:0.0491\n",
      "epoch[143/500] train_loss:0.0500 validation_loss:0.0421\n",
      "epoch[144/500] train_loss:0.0524 validation_loss:0.0488\n",
      "epoch[145/500] train_loss:0.0498 validation_loss:0.0444\n",
      "epoch[146/500] train_loss:0.0493 validation_loss:0.0392\n",
      "epoch[147/500] train_loss:0.0479 validation_loss:0.0421\n",
      "epoch[148/500] train_loss:0.0484 validation_loss:0.0491\n",
      "epoch[149/500] train_loss:0.0480 validation_loss:0.0502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[150/500] train_loss:0.0474 validation_loss:0.0395\n",
      "epoch[151/500] train_loss:0.0459 validation_loss:0.0401\n",
      "epoch[152/500] train_loss:0.0473 validation_loss:0.0367\n",
      "epoch[153/500] train_loss:0.0467 validation_loss:0.0418\n",
      "epoch[154/500] train_loss:0.0456 validation_loss:0.0384\n",
      "epoch[155/500] train_loss:0.0466 validation_loss:0.0513\n",
      "epoch[156/500] train_loss:0.0439 validation_loss:0.0394\n",
      "epoch[157/500] train_loss:0.0421 validation_loss:0.0373\n",
      "epoch[158/500] train_loss:0.0439 validation_loss:0.0385\n",
      "epoch[159/500] train_loss:0.0437 validation_loss:0.0465\n",
      "epoch[160/500] train_loss:0.0464 validation_loss:0.0496\n",
      "epoch[161/500] train_loss:0.0449 validation_loss:0.0564\n",
      "epoch[162/500] train_loss:0.0444 validation_loss:0.0555\n",
      "epoch[163/500] train_loss:0.0459 validation_loss:0.0380\n",
      "epoch[164/500] train_loss:0.0450 validation_loss:0.0470\n",
      "epoch[165/500] train_loss:0.0450 validation_loss:0.0454\n",
      "epoch[166/500] train_loss:0.0451 validation_loss:0.0358\n",
      "epoch[167/500] train_loss:0.0463 validation_loss:0.0343\n",
      "epoch[168/500] train_loss:0.0396 validation_loss:0.0393\n",
      "epoch[169/500] train_loss:0.0471 validation_loss:0.0683\n",
      "epoch[170/500] train_loss:0.0421 validation_loss:0.0524\n",
      "epoch[171/500] train_loss:0.0457 validation_loss:0.0359\n",
      "epoch[172/500] train_loss:0.0424 validation_loss:0.0436\n",
      "epoch[173/500] train_loss:0.0415 validation_loss:0.0360\n",
      "epoch[174/500] train_loss:0.0421 validation_loss:0.0362\n",
      "epoch[175/500] train_loss:0.0444 validation_loss:0.0373\n",
      "epoch[176/500] train_loss:0.0445 validation_loss:0.0334\n",
      "epoch[177/500] train_loss:0.0432 validation_loss:0.0325\n",
      "epoch[178/500] train_loss:0.0427 validation_loss:0.0376\n",
      "epoch[179/500] train_loss:0.0445 validation_loss:0.0337\n",
      "epoch[180/500] train_loss:0.0424 validation_loss:0.0335\n",
      "epoch[181/500] train_loss:0.0420 validation_loss:0.0403\n",
      "epoch[182/500] train_loss:0.0426 validation_loss:0.0348\n",
      "epoch[183/500] train_loss:0.0422 validation_loss:0.0369\n",
      "epoch[184/500] train_loss:0.0417 validation_loss:0.0329\n",
      "epoch[185/500] train_loss:0.0436 validation_loss:0.0371\n",
      "epoch[186/500] train_loss:0.0415 validation_loss:0.1246\n",
      "epoch[187/500] train_loss:0.0420 validation_loss:0.0334\n",
      "epoch[188/500] train_loss:0.0419 validation_loss:0.0408\n",
      "epoch[189/500] train_loss:0.0405 validation_loss:0.0476\n",
      "epoch[190/500] train_loss:0.0396 validation_loss:0.0343\n",
      "epoch[191/500] train_loss:0.0415 validation_loss:0.0343\n",
      "epoch[192/500] train_loss:0.0409 validation_loss:0.0418\n",
      "epoch[193/500] train_loss:0.0427 validation_loss:0.0318\n",
      "epoch[194/500] train_loss:0.0409 validation_loss:0.0315\n",
      "epoch[195/500] train_loss:0.0421 validation_loss:0.0347\n",
      "epoch[196/500] train_loss:0.0422 validation_loss:0.0321\n",
      "epoch[197/500] train_loss:0.0409 validation_loss:0.0370\n",
      "epoch[198/500] train_loss:0.0397 validation_loss:0.0433\n",
      "epoch[199/500] train_loss:0.0415 validation_loss:0.1805\n",
      "epoch[200/500] train_loss:0.0400 validation_loss:0.0655\n",
      "epoch[201/500] train_loss:0.0375 validation_loss:0.0397\n",
      "epoch[202/500] train_loss:0.0416 validation_loss:0.0408\n",
      "epoch[203/500] train_loss:0.0407 validation_loss:0.0356\n",
      "epoch[204/500] train_loss:0.0406 validation_loss:0.0336\n",
      "epoch[205/500] train_loss:0.0387 validation_loss:0.1033\n",
      "epoch[206/500] train_loss:0.0384 validation_loss:0.0424\n",
      "epoch[207/500] train_loss:0.0406 validation_loss:0.0331\n",
      "epoch[208/500] train_loss:0.0404 validation_loss:0.0350\n",
      "epoch[209/500] train_loss:0.0410 validation_loss:0.0529\n",
      "epoch[210/500] train_loss:0.0395 validation_loss:0.0326\n",
      "epoch[211/500] train_loss:0.0380 validation_loss:0.0330\n",
      "epoch[212/500] train_loss:0.0396 validation_loss:0.0312\n",
      "epoch[213/500] train_loss:0.0386 validation_loss:0.0324\n",
      "epoch[214/500] train_loss:0.0390 validation_loss:0.0302\n",
      "epoch[215/500] train_loss:0.0392 validation_loss:0.0332\n",
      "epoch[216/500] train_loss:0.0402 validation_loss:0.0577\n",
      "epoch[217/500] train_loss:0.0366 validation_loss:0.0314\n",
      "epoch[218/500] train_loss:0.0403 validation_loss:0.0334\n",
      "epoch[219/500] train_loss:0.0386 validation_loss:0.0299\n",
      "epoch[220/500] train_loss:0.0404 validation_loss:0.0314\n",
      "epoch[221/500] train_loss:0.0388 validation_loss:0.0342\n",
      "epoch[222/500] train_loss:0.0386 validation_loss:0.0305\n",
      "epoch[223/500] train_loss:0.0385 validation_loss:0.0408\n",
      "epoch[224/500] train_loss:0.0392 validation_loss:0.0312\n",
      "epoch[225/500] train_loss:0.0394 validation_loss:0.0300\n",
      "epoch[226/500] train_loss:0.0377 validation_loss:0.0284\n",
      "epoch[227/500] train_loss:0.0379 validation_loss:0.0389\n",
      "epoch[228/500] train_loss:0.0393 validation_loss:0.0334\n",
      "epoch[229/500] train_loss:0.0385 validation_loss:0.0329\n",
      "epoch[230/500] train_loss:0.0393 validation_loss:0.0280\n",
      "epoch[231/500] train_loss:0.0389 validation_loss:0.0701\n",
      "epoch[232/500] train_loss:0.0370 validation_loss:0.0362\n",
      "epoch[233/500] train_loss:0.0381 validation_loss:0.0294\n",
      "epoch[234/500] train_loss:0.0363 validation_loss:0.0377\n",
      "epoch[235/500] train_loss:0.0386 validation_loss:0.0321\n",
      "epoch[236/500] train_loss:0.0376 validation_loss:0.0316\n",
      "epoch[237/500] train_loss:0.0380 validation_loss:0.0294\n",
      "epoch[238/500] train_loss:0.0384 validation_loss:0.1971\n",
      "epoch[239/500] train_loss:0.0374 validation_loss:0.0387\n",
      "epoch[240/500] train_loss:0.0364 validation_loss:0.0273\n",
      "epoch[241/500] train_loss:0.0367 validation_loss:0.0280\n",
      "epoch[242/500] train_loss:0.0329 validation_loss:0.0274\n",
      "epoch[243/500] train_loss:0.0332 validation_loss:0.0260\n",
      "epoch[244/500] train_loss:0.0358 validation_loss:0.0332\n",
      "epoch[245/500] train_loss:0.0397 validation_loss:0.0282\n",
      "epoch[246/500] train_loss:0.0371 validation_loss:0.0305\n",
      "epoch[247/500] train_loss:0.0373 validation_loss:0.0322\n",
      "epoch[248/500] train_loss:0.0373 validation_loss:0.0604\n",
      "epoch[249/500] train_loss:0.0375 validation_loss:0.0821\n",
      "epoch[250/500] train_loss:0.0351 validation_loss:0.0270\n",
      "epoch[251/500] train_loss:0.0352 validation_loss:0.0276\n",
      "epoch[252/500] train_loss:0.0343 validation_loss:0.0275\n",
      "epoch[253/500] train_loss:0.0368 validation_loss:0.0336\n",
      "epoch[254/500] train_loss:0.0383 validation_loss:0.0350\n",
      "epoch[255/500] train_loss:0.0356 validation_loss:0.0272\n",
      "epoch[256/500] train_loss:0.0359 validation_loss:0.0351\n",
      "epoch[257/500] train_loss:0.0348 validation_loss:0.0419\n",
      "epoch[258/500] train_loss:0.0371 validation_loss:0.0279\n",
      "epoch[259/500] train_loss:0.0373 validation_loss:0.0267\n",
      "epoch[260/500] train_loss:0.0353 validation_loss:0.0284\n",
      "epoch[261/500] train_loss:0.0339 validation_loss:0.0271\n",
      "epoch[262/500] train_loss:0.0333 validation_loss:0.0270\n",
      "epoch[263/500] train_loss:0.0363 validation_loss:0.0263\n",
      "epoch[264/500] train_loss:0.0351 validation_loss:0.0324\n",
      "epoch[265/500] train_loss:0.0357 validation_loss:0.0311\n",
      "epoch[266/500] train_loss:0.0340 validation_loss:0.0257\n",
      "epoch[267/500] train_loss:0.0344 validation_loss:0.0260\n",
      "epoch[268/500] train_loss:0.0351 validation_loss:0.0284\n",
      "epoch[269/500] train_loss:0.0344 validation_loss:0.0291\n",
      "epoch[270/500] train_loss:0.0363 validation_loss:0.0248\n",
      "epoch[271/500] train_loss:0.0344 validation_loss:0.0271\n",
      "epoch[272/500] train_loss:0.0355 validation_loss:0.0249\n",
      "epoch[273/500] train_loss:0.0360 validation_loss:0.0325\n",
      "epoch[274/500] train_loss:0.0333 validation_loss:0.0282\n",
      "epoch[275/500] train_loss:0.0340 validation_loss:0.0266\n",
      "epoch[276/500] train_loss:0.0358 validation_loss:0.0303\n",
      "epoch[277/500] train_loss:0.0347 validation_loss:0.0350\n",
      "epoch[278/500] train_loss:0.0335 validation_loss:0.0335\n",
      "epoch[279/500] train_loss:0.0343 validation_loss:0.1930\n",
      "epoch[280/500] train_loss:0.0345 validation_loss:0.2285\n",
      "epoch[281/500] train_loss:0.0347 validation_loss:0.0634\n",
      "epoch[282/500] train_loss:0.0334 validation_loss:0.0327\n",
      "epoch[283/500] train_loss:0.0339 validation_loss:0.0467\n",
      "epoch[284/500] train_loss:0.0328 validation_loss:0.0401\n",
      "epoch[285/500] train_loss:0.0348 validation_loss:0.0272\n",
      "epoch[286/500] train_loss:0.0319 validation_loss:0.0274\n",
      "epoch[287/500] train_loss:0.0354 validation_loss:0.0281\n",
      "epoch[288/500] train_loss:0.0343 validation_loss:0.0306\n",
      "epoch[289/500] train_loss:0.0362 validation_loss:0.0456\n",
      "epoch[290/500] train_loss:0.0323 validation_loss:0.0303\n",
      "epoch[291/500] train_loss:0.0361 validation_loss:0.0337\n",
      "epoch[292/500] train_loss:0.0323 validation_loss:0.0404\n",
      "epoch[293/500] train_loss:0.0327 validation_loss:0.0289\n",
      "epoch[294/500] train_loss:0.0346 validation_loss:0.0347\n",
      "epoch[295/500] train_loss:0.0326 validation_loss:0.0268\n",
      "epoch[296/500] train_loss:0.0339 validation_loss:0.0303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[297/500] train_loss:0.0329 validation_loss:0.0390\n",
      "epoch[298/500] train_loss:0.0314 validation_loss:0.0260\n",
      "epoch[299/500] train_loss:0.0328 validation_loss:0.0317\n",
      "epoch[300/500] train_loss:0.0348 validation_loss:0.0280\n",
      "epoch[301/500] train_loss:0.0332 validation_loss:0.0320\n",
      "epoch[302/500] train_loss:0.0353 validation_loss:0.0254\n",
      "epoch[303/500] train_loss:0.0336 validation_loss:0.1581\n",
      "epoch[304/500] train_loss:0.0317 validation_loss:0.0381\n",
      "epoch[305/500] train_loss:0.0337 validation_loss:0.0334\n",
      "epoch[306/500] train_loss:0.0339 validation_loss:0.0251\n",
      "epoch[307/500] train_loss:0.0329 validation_loss:0.0265\n",
      "epoch[308/500] train_loss:0.0306 validation_loss:0.0306\n",
      "epoch[309/500] train_loss:0.0327 validation_loss:0.0304\n",
      "epoch[310/500] train_loss:0.0335 validation_loss:0.0244\n",
      "epoch[311/500] train_loss:0.0331 validation_loss:0.0287\n",
      "epoch[312/500] train_loss:0.0333 validation_loss:0.0283\n",
      "epoch[313/500] train_loss:0.0338 validation_loss:0.0313\n",
      "epoch[314/500] train_loss:0.0350 validation_loss:0.0310\n",
      "epoch[315/500] train_loss:0.0315 validation_loss:0.0271\n",
      "epoch[316/500] train_loss:0.0343 validation_loss:0.0249\n",
      "epoch[317/500] train_loss:0.0326 validation_loss:0.0269\n",
      "epoch[318/500] train_loss:0.0323 validation_loss:0.0259\n",
      "epoch[319/500] train_loss:0.0325 validation_loss:0.0239\n",
      "epoch[320/500] train_loss:0.0328 validation_loss:0.0265\n",
      "epoch[321/500] train_loss:0.0319 validation_loss:0.0256\n",
      "epoch[322/500] train_loss:0.0344 validation_loss:0.0238\n",
      "epoch[323/500] train_loss:0.0338 validation_loss:0.0424\n",
      "epoch[324/500] train_loss:0.0329 validation_loss:0.0512\n",
      "epoch[325/500] train_loss:0.0326 validation_loss:0.0275\n",
      "epoch[326/500] train_loss:0.0313 validation_loss:0.0305\n",
      "epoch[327/500] train_loss:0.0327 validation_loss:0.0284\n",
      "epoch[328/500] train_loss:0.0318 validation_loss:0.0261\n",
      "epoch[329/500] train_loss:0.0317 validation_loss:0.0257\n",
      "epoch[330/500] train_loss:0.0321 validation_loss:0.0261\n",
      "epoch[331/500] train_loss:0.0311 validation_loss:0.0259\n",
      "epoch[332/500] train_loss:0.0311 validation_loss:0.0465\n",
      "epoch[333/500] train_loss:0.0311 validation_loss:0.0309\n",
      "epoch[334/500] train_loss:0.0336 validation_loss:0.0281\n",
      "epoch[335/500] train_loss:0.0320 validation_loss:0.0273\n",
      "epoch[336/500] train_loss:0.0311 validation_loss:0.0369\n",
      "epoch[337/500] train_loss:0.0304 validation_loss:0.0329\n",
      "epoch[338/500] train_loss:0.0297 validation_loss:0.0238\n",
      "epoch[339/500] train_loss:0.0304 validation_loss:0.0274\n",
      "epoch[340/500] train_loss:0.0323 validation_loss:0.0241\n",
      "epoch[341/500] train_loss:0.0309 validation_loss:0.0248\n",
      "epoch[342/500] train_loss:0.0317 validation_loss:0.0238\n",
      "epoch[343/500] train_loss:0.0312 validation_loss:0.0262\n",
      "epoch[344/500] train_loss:0.0299 validation_loss:0.0223\n",
      "epoch[345/500] train_loss:0.0322 validation_loss:0.0259\n",
      "epoch[346/500] train_loss:0.0328 validation_loss:0.0239\n",
      "epoch[347/500] train_loss:0.0312 validation_loss:0.0267\n",
      "epoch[348/500] train_loss:0.0305 validation_loss:0.0234\n",
      "epoch[349/500] train_loss:0.0324 validation_loss:0.0238\n",
      "epoch[350/500] train_loss:0.0319 validation_loss:0.0434\n",
      "epoch[351/500] train_loss:0.0331 validation_loss:0.0239\n",
      "epoch[352/500] train_loss:0.0310 validation_loss:0.0291\n",
      "epoch[353/500] train_loss:0.0299 validation_loss:0.0261\n",
      "epoch[354/500] train_loss:0.0322 validation_loss:0.0245\n",
      "epoch[355/500] train_loss:0.0289 validation_loss:0.0245\n",
      "epoch[356/500] train_loss:0.0323 validation_loss:0.0229\n",
      "epoch[357/500] train_loss:0.0300 validation_loss:0.0247\n",
      "epoch[358/500] train_loss:0.0293 validation_loss:0.0251\n",
      "epoch[359/500] train_loss:0.0298 validation_loss:0.0228\n",
      "epoch[360/500] train_loss:0.0303 validation_loss:0.0228\n",
      "epoch[361/500] train_loss:0.0313 validation_loss:0.0226\n",
      "epoch[362/500] train_loss:0.0292 validation_loss:0.0230\n",
      "epoch[363/500] train_loss:0.0299 validation_loss:0.0234\n",
      "epoch[364/500] train_loss:0.0287 validation_loss:0.0292\n",
      "epoch[365/500] train_loss:0.0299 validation_loss:0.0228\n",
      "epoch[366/500] train_loss:0.0313 validation_loss:0.0270\n",
      "epoch[367/500] train_loss:0.0308 validation_loss:0.0388\n",
      "epoch[368/500] train_loss:0.0287 validation_loss:0.0243\n",
      "epoch[369/500] train_loss:0.0301 validation_loss:0.0244\n",
      "epoch[370/500] train_loss:0.0324 validation_loss:0.0481\n",
      "epoch[371/500] train_loss:0.0350 validation_loss:0.1464\n",
      "epoch[372/500] train_loss:0.0325 validation_loss:0.1265\n",
      "epoch[373/500] train_loss:0.0331 validation_loss:0.1228\n",
      "epoch[374/500] train_loss:0.0309 validation_loss:0.0795\n",
      "epoch[375/500] train_loss:0.0317 validation_loss:0.0363\n",
      "epoch[376/500] train_loss:0.0313 validation_loss:0.0290\n",
      "epoch[377/500] train_loss:0.0306 validation_loss:0.0302\n",
      "epoch[378/500] train_loss:0.0315 validation_loss:0.0245\n",
      "epoch[379/500] train_loss:0.0330 validation_loss:0.0246\n",
      "epoch[380/500] train_loss:0.0298 validation_loss:0.0257\n",
      "epoch[381/500] train_loss:0.0305 validation_loss:0.0251\n",
      "epoch[382/500] train_loss:0.0312 validation_loss:0.0237\n",
      "epoch[383/500] train_loss:0.0306 validation_loss:0.0230\n",
      "epoch[384/500] train_loss:0.0307 validation_loss:0.0252\n",
      "epoch[385/500] train_loss:0.0294 validation_loss:0.0314\n",
      "epoch[386/500] train_loss:0.0296 validation_loss:0.0232\n",
      "epoch[387/500] train_loss:0.0297 validation_loss:0.0305\n",
      "epoch[388/500] train_loss:0.0305 validation_loss:0.0244\n",
      "epoch[389/500] train_loss:0.0295 validation_loss:0.0271\n",
      "epoch[390/500] train_loss:0.0326 validation_loss:0.0535\n",
      "epoch[391/500] train_loss:0.0299 validation_loss:0.0297\n",
      "epoch[392/500] train_loss:0.0321 validation_loss:0.0487\n",
      "epoch[393/500] train_loss:0.0316 validation_loss:0.0297\n",
      "epoch[394/500] train_loss:0.0302 validation_loss:0.0257\n",
      "epoch[395/500] train_loss:0.0307 validation_loss:0.0238\n",
      "epoch[396/500] train_loss:0.0311 validation_loss:0.0324\n",
      "epoch[397/500] train_loss:0.0274 validation_loss:0.0284\n",
      "epoch[398/500] train_loss:0.0323 validation_loss:0.0233\n",
      "epoch[399/500] train_loss:0.0286 validation_loss:0.0243\n",
      "epoch[400/500] train_loss:0.0301 validation_loss:0.0219\n",
      "epoch[401/500] train_loss:0.0286 validation_loss:0.0221\n",
      "epoch[402/500] train_loss:0.0285 validation_loss:0.0220\n",
      "epoch[403/500] train_loss:0.0289 validation_loss:0.0243\n",
      "epoch[404/500] train_loss:0.0329 validation_loss:0.0216\n",
      "epoch[405/500] train_loss:0.0284 validation_loss:0.0250\n",
      "epoch[406/500] train_loss:0.0316 validation_loss:0.0282\n",
      "epoch[407/500] train_loss:0.0306 validation_loss:0.0236\n",
      "epoch[408/500] train_loss:0.0296 validation_loss:0.0224\n",
      "epoch[409/500] train_loss:0.0308 validation_loss:0.0227\n",
      "epoch[410/500] train_loss:0.0288 validation_loss:0.0262\n",
      "epoch[411/500] train_loss:0.0288 validation_loss:0.0304\n",
      "epoch[412/500] train_loss:0.0289 validation_loss:0.0217\n",
      "epoch[413/500] train_loss:0.0293 validation_loss:0.0251\n",
      "epoch[414/500] train_loss:0.0279 validation_loss:0.0239\n",
      "epoch[415/500] train_loss:0.0288 validation_loss:0.0231\n",
      "epoch[416/500] train_loss:0.0303 validation_loss:0.0224\n",
      "epoch[417/500] train_loss:0.0296 validation_loss:0.0242\n",
      "epoch[418/500] train_loss:0.0288 validation_loss:0.0212\n",
      "epoch[419/500] train_loss:0.0282 validation_loss:0.0211\n",
      "epoch[420/500] train_loss:0.0280 validation_loss:0.0214\n",
      "epoch[421/500] train_loss:0.0271 validation_loss:0.0214\n",
      "epoch[422/500] train_loss:0.0287 validation_loss:0.0220\n",
      "epoch[423/500] train_loss:0.0301 validation_loss:0.0227\n",
      "epoch[424/500] train_loss:0.0284 validation_loss:0.0215\n",
      "epoch[425/500] train_loss:0.0287 validation_loss:0.0282\n",
      "epoch[426/500] train_loss:0.0303 validation_loss:0.0213\n",
      "epoch[427/500] train_loss:0.0296 validation_loss:0.0272\n",
      "epoch[428/500] train_loss:0.0297 validation_loss:0.0264\n",
      "epoch[429/500] train_loss:0.0263 validation_loss:0.0234\n",
      "epoch[430/500] train_loss:0.0311 validation_loss:0.0213\n",
      "epoch[431/500] train_loss:0.0282 validation_loss:0.0251\n",
      "epoch[432/500] train_loss:0.0283 validation_loss:0.0210\n",
      "epoch[433/500] train_loss:0.0286 validation_loss:0.0219\n",
      "epoch[434/500] train_loss:0.0266 validation_loss:0.0253\n",
      "epoch[435/500] train_loss:0.0272 validation_loss:0.0223\n",
      "epoch[436/500] train_loss:0.0292 validation_loss:0.0240\n",
      "epoch[437/500] train_loss:0.0283 validation_loss:0.0312\n",
      "epoch[438/500] train_loss:0.0302 validation_loss:0.0256\n",
      "epoch[439/500] train_loss:0.0293 validation_loss:0.0217\n",
      "epoch[440/500] train_loss:0.0295 validation_loss:0.0207\n",
      "epoch[441/500] train_loss:0.0264 validation_loss:0.0222\n",
      "epoch[442/500] train_loss:0.0280 validation_loss:0.0210\n",
      "epoch[443/500] train_loss:0.0285 validation_loss:0.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[444/500] train_loss:0.0294 validation_loss:0.0218\n",
      "epoch[445/500] train_loss:0.0291 validation_loss:0.0220\n",
      "epoch[446/500] train_loss:0.0286 validation_loss:0.0208\n",
      "epoch[447/500] train_loss:0.0285 validation_loss:0.0203\n",
      "epoch[448/500] train_loss:0.0260 validation_loss:0.0208\n",
      "epoch[449/500] train_loss:0.0292 validation_loss:0.0213\n",
      "epoch[450/500] train_loss:0.0273 validation_loss:0.0211\n",
      "epoch[451/500] train_loss:0.0282 validation_loss:0.0225\n",
      "epoch[452/500] train_loss:0.0278 validation_loss:0.0202\n",
      "epoch[453/500] train_loss:0.0274 validation_loss:0.0243\n",
      "epoch[454/500] train_loss:0.0286 validation_loss:0.0230\n",
      "epoch[455/500] train_loss:0.0269 validation_loss:0.0211\n",
      "epoch[456/500] train_loss:0.0287 validation_loss:0.0215\n",
      "epoch[457/500] train_loss:0.0290 validation_loss:0.0206\n",
      "epoch[458/500] train_loss:0.0272 validation_loss:0.0248\n",
      "epoch[459/500] train_loss:0.0287 validation_loss:0.0219\n",
      "epoch[460/500] train_loss:0.0295 validation_loss:0.1215\n",
      "epoch[461/500] train_loss:0.0280 validation_loss:0.0207\n",
      "epoch[462/500] train_loss:0.0275 validation_loss:0.0262\n",
      "epoch[463/500] train_loss:0.0292 validation_loss:0.0344\n",
      "epoch[464/500] train_loss:0.0286 validation_loss:0.0219\n",
      "epoch[465/500] train_loss:0.0290 validation_loss:0.0214\n",
      "epoch[466/500] train_loss:0.0274 validation_loss:0.0235\n",
      "epoch[467/500] train_loss:0.0283 validation_loss:0.0226\n",
      "epoch[468/500] train_loss:0.0276 validation_loss:0.0215\n",
      "epoch[469/500] train_loss:0.0291 validation_loss:0.0221\n",
      "epoch[470/500] train_loss:0.0274 validation_loss:0.0230\n",
      "epoch[471/500] train_loss:0.0277 validation_loss:0.0220\n",
      "epoch[472/500] train_loss:0.0269 validation_loss:0.0262\n",
      "epoch[473/500] train_loss:0.0284 validation_loss:0.0268\n",
      "epoch[474/500] train_loss:0.0275 validation_loss:0.0206\n",
      "epoch[475/500] train_loss:0.0256 validation_loss:0.0249\n",
      "epoch[476/500] train_loss:0.0275 validation_loss:0.0221\n",
      "epoch[477/500] train_loss:0.0289 validation_loss:0.0282\n",
      "epoch[478/500] train_loss:0.0271 validation_loss:0.0243\n",
      "epoch[479/500] train_loss:0.0265 validation_loss:0.0233\n",
      "epoch[480/500] train_loss:0.0274 validation_loss:0.0207\n",
      "epoch[481/500] train_loss:0.0273 validation_loss:0.0203\n",
      "epoch[482/500] train_loss:0.0281 validation_loss:0.0203\n",
      "epoch[483/500] train_loss:0.0287 validation_loss:0.0220\n",
      "epoch[484/500] train_loss:0.0273 validation_loss:0.0205\n",
      "epoch[485/500] train_loss:0.0280 validation_loss:0.0207\n",
      "epoch[486/500] train_loss:0.0277 validation_loss:0.0238\n",
      "epoch[487/500] train_loss:0.0268 validation_loss:0.0209\n",
      "epoch[488/500] train_loss:0.0269 validation_loss:0.0206\n",
      "epoch[489/500] train_loss:0.0276 validation_loss:0.0292\n",
      "epoch[490/500] train_loss:0.0262 validation_loss:0.0203\n",
      "epoch[491/500] train_loss:0.0278 validation_loss:0.0228\n",
      "epoch[492/500] train_loss:0.0262 validation_loss:0.0242\n",
      "epoch[493/500] train_loss:0.0282 validation_loss:0.0257\n",
      "epoch[494/500] train_loss:0.0276 validation_loss:0.0229\n",
      "epoch[495/500] train_loss:0.0274 validation_loss:0.0207\n",
      "epoch[496/500] train_loss:0.0270 validation_loss:0.0230\n",
      "epoch[497/500] train_loss:0.0261 validation_loss:0.0228\n",
      "epoch[498/500] train_loss:0.0286 validation_loss:0.0231\n",
      "epoch[499/500] train_loss:0.0265 validation_loss:0.0210\n",
      "epoch[500/500] train_loss:0.0263 validation_loss:0.0243\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f7373fc18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(validation_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ64, 学習率5e-4というのが1つのうまく行く組合せらしい（他にもあると思うが、まだ見つけられていない）\n",
    "# 過学習を起こしている様子はあまりないが、validation lossの安定感に非常に欠ける\n",
    "# フィルタサイズは大きい方が性能が良い（当然といえば当然） 16とかだとマジで結果が悪い\n",
    "# 学習率のグラフで結果がたまに跳ね上がることがあるが、それは恐らく絵の画像に突き当たったりしているため"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
