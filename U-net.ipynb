{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.1\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "\n",
    "# batchsize\n",
    "batchsize = 5\n",
    "\n",
    "# imagesize for random crop\n",
    "imagesize = 256\n",
    "\n",
    "# seed setting (warning : cuDNN's randomness is remaining)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "# directory settings\n",
    "# data directory\n",
    "root_dir = '../../data/200003076/'\n",
    "image_dir = root_dir + 'images_resized_512/'\n",
    "label_dir = root_dir + 'labels_resized_512/gaussian_0.05/'\n",
    "\n",
    "# directory to put generated images\n",
    "output_dir = root_dir + 'output_resized_512_gaussian_0.05/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "# directory to save state_dict and loss.npy\n",
    "save_dir = root_dir + 'save_resized_512_gaussian_0.05/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset class for image loading\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform_image = None, transform_label = None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        self.image_list = os.listdir(image_dir)\n",
    "        self.label_list = os.listdir(label_dir)\n",
    "        \n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.image_list[idx]\n",
    "        label_name = self.label_dir + self.label_list[idx]\n",
    "        \n",
    "        image = io.imread(image_name)\n",
    "        label = io.imread(label_name)\n",
    "        \n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "            \n",
    "        if self.transform_label:\n",
    "            label = self.transform_label(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transform\n",
    "# Normalize [0~255] to [0~1]\n",
    "class Normalize:\n",
    "    def __call__(self, image):\n",
    "        return image / 255\n",
    "    \n",
    "class Add_dim:\n",
    "    def __call__(self, label):\n",
    "        return label.reshape(label.shape[0], label.shape[1], 1)\n",
    "    \n",
    "class Tofloat:\n",
    "    def __call__(self, tensor):\n",
    "        return tensor.float()\n",
    "    \n",
    "tf_image = transforms.Compose([Normalize(), transforms.ToTensor(), Tofloat()])\n",
    "tf_label = transforms.Compose([Add_dim(), transforms.ToTensor(), Tofloat()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# make dataset\n",
    "imgDataset = MyDataset(image_dir, label_dir, transform_image = tf_image, transform_label = tf_label)\n",
    "\n",
    "# split to train data and validation data\n",
    "train_data, validation_data = train_test_split(imgDataset, test_size = 0.2, random_state = seed)\n",
    "\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of validation data:', len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = batchsize, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parts for U-net for convenience\n",
    "# downsampling\n",
    "# conv > batchnorm > dropout > leakyrelu\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 4 , stride = 2, padding = 1, \\\n",
    "                                                                    use_batchnorm = True, use_dropout = False):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.3)\n",
    "        self.rl = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "            \n",
    "        out = self.rl(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parts for U-net for convenience\n",
    "# upsampling (using transposed convolution)\n",
    "# conv > batchnorm > dropout > relu\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1, \\\n",
    "                                                                   use_batchnorm = True, use_dropout = False):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dr = nn.Dropout(0.3)\n",
    "        self.rl = nn.ReLU()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        \n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "            \n",
    "        out = self.rl(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Memo : CNN size equation (no dilation)\n",
    "\n",
    "                                OUT = (IN + 2*Padding - Kernel_size) / Stride + 1                 \n",
    "'''\n",
    "# U-net architecture\n",
    "class U_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(U_net, self).__init__()\n",
    "        \n",
    "        # U-net encoder\n",
    "        # default: kernel_size = 4, stride = 2, padding = 1, using batchnorm, no dropout\n",
    "        self.encoder1 = Downsample(  3,   64, use_batchnorm = False)   # out tensor size: (batchsize,   64, 256, 256)\n",
    "        self.encoder2 = Downsample( 64,  128)                          # out tensor size: (batchsize,  128, 128, 128)\n",
    "        self.encoder3 = Downsample(128,  256)                          # out tensor size: (batchsize,  256,  64,  64)\n",
    "        self.encoder4 = Downsample(256,  512)                          # out tensor size: (batchsize,  512,  32,  32)\n",
    "        self.encoder5 = Downsample(512,  512)                          # out tensor size: (batchsize,  512,  16,  16)\n",
    "        self.encoder6 = Downsample(512,  512)                          # out tensor size: (batchsize,  512,   8,   8)\n",
    "        self.encoder7 = Downsample(512,  512)                          # out tensor size: (batchsize,  512,   4,   4)\n",
    "        self.encoder8 = Downsample(512,  512)                          # out tensor size: (batchsize,  512,   2,   2)\n",
    "        self.encoder9 = Downsample(512,  1024)                         # out tensor size: (batchsize, 1024\\\\\\\\\\\\\\\\\\,   1,   1)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # U-net decoder\n",
    "        # default: kernel_size = 4, stride = 2, padding = 1, using batchnorm, no dropout\n",
    "        self.decoder1 = Upsample(1024    , 1024, use_dropout = True)    # out tensor size: (batchsize, 1024,   2,   2)\n",
    "        self.decoder2 = Upsample(1024+512, 1024, use_dropout = True)    # out tensor size: (batchsize, 1024,   4,   4)\n",
    "        self.decoder3 = Upsample(1024+512, 1024, use_dropout = True)    # out tensor size: (batchsize, 1024,   8,   8)\n",
    "        self.decoder4 = Upsample(1024+512, 1024)                        # out tensor size: (batchsize, 1024,  16,  16)\n",
    "        self.decoder5 = Upsample(1024+512, 1024)                        # out tensor size: (batchsize, 1024,  32,  32)\n",
    "        self.decoder6 = Upsample(1024+512, 1024)                        # out tensor size: (batchsize, 1024,  64,  64)\n",
    "        self.decoder7 = Upsample(1024+256,  512)                        # out tensor size: (batchsize,  512, 128, 128)\n",
    "        self.decoder8 = Upsample( 512+128,  256)                        # out tensor size: (batchsize,  256, 256, 256)\n",
    "        self.decoder9 = Upsample( 256+ 64,  128)                        # out tensor size: (batchsize,  128, 512, 512)\n",
    "        \n",
    "        # pointwise convolution to adjust channel with no image size change\n",
    "        self.decoder10 = nn.Sequential(\n",
    "                            nn.Conv2d(128, 64, kernel_size = 1, stride = 1, padding = 0),\n",
    "                            nn.BatchNorm2d(64),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(64, 32, kernel_size = 1, stride = 1, padding = 0),\n",
    "                            nn.BatchNorm2d(32),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(32, 1, kernel_size = 1, stride = 1, padding = 0),\n",
    "                            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoding part\n",
    "        out_encoder1 = self.encoder1(x)\n",
    "        out_encoder2 = self.encoder2(out_encoder1)\n",
    "        out_encoder3 = self.encoder3(out_encoder2)\n",
    "        out_encoder4 = self.encoder4(out_encoder3)\n",
    "        out_encoder5 = self.encoder5(out_encoder4)\n",
    "        out_encoder6 = self.encoder6(out_encoder5)\n",
    "        out_encoder7 = self.encoder7(out_encoder6)\n",
    "        out_encoder8 = self.encoder8(out_encoder7)\n",
    "        out_encoder9 = self.encoder9(out_encoder8)\n",
    "        \n",
    "        # decording part\n",
    "        out_decoder1 = self.decoder1(out_encoder9)\n",
    "        out_decoder2 = self.decoder2(torch.cat([out_decoder1, out_encoder8], dim = 1))\n",
    "        out_decoder3 = self.decoder3(torch.cat([out_decoder2, out_encoder7], dim = 1))\n",
    "        out_decoder4 = self.decoder4(torch.cat([out_decoder3, out_encoder6], dim = 1))\n",
    "        out_decoder5 = self.decoder5(torch.cat([out_decoder4, out_encoder5], dim = 1))\n",
    "        out_decoder6 = self.decoder6(torch.cat([out_decoder5, out_encoder4], dim = 1))\n",
    "        out_decoder7 = self.decoder7(torch.cat([out_decoder6, out_encoder3], dim = 1))\n",
    "        out_decoder8 = self.decoder8(torch.cat([out_decoder7, out_encoder2], dim = 1))\n",
    "        out_decoder9 = self.decoder9(torch.cat([out_decoder8, out_encoder1], dim = 1))\n",
    "        \n",
    "        out = self.decoder10(out_decoder9)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network, optimizer and hyperparameters settings\n",
    "\n",
    "# instantiate networks\n",
    "u_net = U_net()\n",
    "\n",
    "# send to GPU(CPU)\n",
    "u_net = u_net.to(device)\n",
    "\n",
    "# set optimizer\n",
    "u_net_optimizer = optim.Adam(u_net.parameters(), lr = 0.0002) #, betas = [0.5, 0.999])\n",
    "\n",
    "# init weights\n",
    "for p in u_net.parameters():\n",
    "    nn.init.normal_(p, mean = 0, std = 0.02)\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params_u_net = sum(p.numel() for p in u_net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-net\n",
      "The number of trainable parameters: 184334081\n",
      "\n",
      "Model\n",
      " U_net(\n",
      "  (encoder1): Downsample(\n",
      "    (cv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder2): Downsample(\n",
      "    (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder3): Downsample(\n",
      "    (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder4): Downsample(\n",
      "    (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder5): Downsample(\n",
      "    (cv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder6): Downsample(\n",
      "    (cv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder7): Downsample(\n",
      "    (cv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder8): Downsample(\n",
      "    (cv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (encoder9): Downsample(\n",
      "    (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (decoder1): Upsample(\n",
      "    (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder2): Upsample(\n",
      "    (tc): ConvTranspose2d(1536, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder3): Upsample(\n",
      "    (tc): ConvTranspose2d(1536, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder4): Upsample(\n",
      "    (tc): ConvTranspose2d(1536, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder5): Upsample(\n",
      "    (tc): ConvTranspose2d(1536, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder6): Upsample(\n",
      "    (tc): ConvTranspose2d(1536, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder7): Upsample(\n",
      "    (tc): ConvTranspose2d(1280, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder8): Upsample(\n",
      "    (tc): ConvTranspose2d(640, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder9): Upsample(\n",
      "    (tc): ConvTranspose2d(320, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dr): Dropout(p=0.3)\n",
      "    (rl): ReLU()\n",
      "  )\n",
      "  (decoder10): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Optimizer\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print settings\n",
    "print('U-net')\n",
    "print('The number of trainable parameters:', num_trainable_params_u_net)\n",
    "print('\\nModel\\n', u_net)\n",
    "print('\\nOptimizer\\n', u_net_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    u_net.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # calculate network outputs\n",
    "        outputs = u_net(inputs)\n",
    "        \n",
    "        # calculate loss function, run backward calculation, and update weights\n",
    "        u_net_optimizer.zero_grad()\n",
    "        u_net_loss = F.smooth_l1_loss(outputs, labels)\n",
    "        u_net.backward()\n",
    "        u_net_optimizer.step()\n",
    "        \n",
    "        running_loss += u_net_loss.item()\n",
    "        \n",
    "    # devide by len(data_loader) because F.smooth_l1_loss is normalized in minibatch\n",
    "    loss = running_loss / len(data_loader)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_save_images = 5\n",
    "interval_save_images = 1 # epoch\n",
    "\n",
    "def validation(data_loader, epoch):\n",
    "    u_net.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # calculate network outputs\n",
    "            outputs = u_net(inputs)\n",
    "            \n",
    "            running_loss += F.smooth_l1_loss(outputs, labels).item()\n",
    "\n",
    "            \n",
    "    # save [n_save_images] (input, label, output) comparison image\n",
    "    if epoch % interval_save_images == 0:\n",
    "        for n in range(n_save_images):\n",
    "            input_image = inputs[n].unsqueeze(0)\n",
    "            label = labels[n].unsqueeze(0)\n",
    "            output = outputs[n].unsqueeze(0)\n",
    "            comparison = torch.cat([input_image, label, output])\n",
    "            save_image(comparison.data.cpu(), '{}/{}_{}.png'.format(output_dir, epoch, n))\n",
    "                    \n",
    "    \n",
    "    loss = running_loss / len(data_loader)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dafb24f0150d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-fc6b19348f02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# calculate network outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# calculate loss function, run backward calculation, and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-454e185aae4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mout_decoder7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_decoder6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_encoder3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout_decoder8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_decoder7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_encoder2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout_decoder9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_decoder8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_encoder1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_decoder9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-af6bfe285618>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_batchnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1253\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%d/%d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss)) \n",
    "\n",
    "# save state_dicts\n",
    "torch.save(u_net.state_dict(), save_dir + 'u_net_' + str(epoch) + '.pth')\n",
    "torch.save(u_net_optimizer.state_dict(), save_dir + 'u_net_optmizer_' + str(epoch) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(save_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(save_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
