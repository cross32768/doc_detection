{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:add argument system to change experiment condition for structure expolation and do experiments for paper\n",
    "import os\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "can_use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', can_use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "\n",
    "device = torch.device('cuda' if can_use_gpu else 'cpu')\n",
    "\n",
    "batchsize_train = 64\n",
    "batchsize_validation = 5 # this also means the number of images saved in every interval epoch.\n",
    "\n",
    "height_for_train_cropping = 128\n",
    "width_for_train_cropping = 128\n",
    "height_for_validation_cropping = 768\n",
    "width_for_validation_cropping = 512\n",
    "\n",
    "# TODO:seed setting and exclude randomness?\n",
    "\n",
    "# directory settings\n",
    "root_dir = '../../data/komonjo_experiment/200003076/'\n",
    "\n",
    "# training data directory\n",
    "image_dir = root_dir + 'training_data/images_resized_quarter/'\n",
    "label_dir = root_dir + 'training_data/one_hot_x0.8_resized_quarter/'\n",
    "\n",
    "result_dir = root_dir + 'experiment_result/'\n",
    "conducted_experiment_name_list = sorted(os.listdir(result_dir))\n",
    "new_experiment_name = 'experiment_%03d' % (int(conducted_experiment_name_list[-1].split('_')[1])+1)\n",
    "new_experiment_dir = result_dir + new_experiment_name + '/'\n",
    "os.mkdir(new_experiment_dir)\n",
    "\n",
    "# directory to save model output\n",
    "result_image_dir = new_experiment_dir + 'result_image/'\n",
    "if not os.path.exists(result_image_dir):\n",
    "    os.mkdir(result_image_dir)\n",
    "\n",
    "# directory to save model weights and training log\n",
    "log_dir = new_experiment_dir + 'trained_weights_and_training_log/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "experiment_condition_txt = new_experiment_dir + 'experiment_condition.txt'\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('{} condition\\n\\n'.format(new_experiment_name))\n",
    "    f.write('PyTorch version : {}\\n'.format(torch.__version__))\n",
    "    f.write('torchvision version : {}\\n\\n'.format(torchvision.__version__))\n",
    "    f.write('training batchsize : {}\\n'.format(batchsize_train))\n",
    "    f.write('training crop size : {}\\n'.format((height_for_train_cropping, width_for_train_cropping)))\n",
    "    f.write('validation crop size : {}\\n\\n'.format((height_for_validation_cropping, width_for_validation_cropping)))\n",
    "    f.write('used image dataset : {}\\n'.format(image_dir))\n",
    "    f.write('used label dataset : {}\\n\\n'.format(label_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_name_list,\n",
    "                 transform_sync=None, transform_image=None, transform_label=None):\n",
    "        assert(image_dir[-1] == '/')\n",
    "        assert(label_dir[-1] == '/')\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # image or label filename list in image_dir or label_dir (to speedup train_test_split, I'll split file name list)\n",
    "        # I expect corresponding image and label have same filename\n",
    "        # This sort is so that following __getitem__ method expect file_name_list have unique order\n",
    "        self.file_name_list = sorted(file_name_list) \n",
    "        \n",
    "        # to do same random cropping for corresponding image and label\n",
    "        self.transform_sync = transform_sync\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_dir + self.file_name_list[idx]\n",
    "        label_name = self.label_dir + self.file_name_list[idx]\n",
    "        label_name = label_name.replace('.jpg', '.png')\n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        label = Image.open(label_name)\n",
    "        \n",
    "        if self.transform_sync is not None:\n",
    "            image, label = self.transform_sync(image, label)\n",
    "        if self.transform_image is not None:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_label is not None:\n",
    "            label = self.transform_label(label) \n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 276\n",
      "The number of validation data: 70\n"
     ]
    }
   ],
   "source": [
    "# split to train data and validation data for simplicity\n",
    "# TODO:test should be conducted by isolated test data (different document)\n",
    "\n",
    "# sort to eliminate os.listdir randomness\n",
    "# I expect corresponding image and label have same filename\n",
    "file_name = sorted(os.listdir(image_dir))\n",
    "train_file_name, validation_file_name = train_test_split(file_name, test_size=0.2)\n",
    "\n",
    "print('The number of training data:', len(train_file_name))\n",
    "print('The number of validation data:', len(validation_file_name))\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of training data : {}\\n'.format(len(train_file_name)))\n",
    "    f.write('The number of validation data : {}\\n\\n'.format(len(validation_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for synchronize cropping for image and label\n",
    "# warning:this class can't do padding\n",
    "class RandomCropSync(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(self, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        \n",
    "        i = np.random.randint(0, h - th)\n",
    "        j = np.random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "    \n",
    "    def __call__(self, img1, img2):\n",
    "        assert(img1.size == img2.size)\n",
    "        i, j, h, w = self.get_params(img1, self.size)\n",
    "        \n",
    "        img1_cropped = torchvision.transforms.functional.crop(img1, i, j, h, w)\n",
    "        img2_cropped = torchvision.transforms.functional.crop(img2, i, j, h, w)\n",
    "        \n",
    "        return img1_cropped, img2_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sync_train = RandomCropSync((height_for_train_cropping, width_for_train_cropping))\n",
    "tf_image_train = transforms.ToTensor()\n",
    "tf_label_train = transforms.ToTensor()\n",
    "tf_image_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping,\n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "tf_label_validation = transforms.Compose([transforms.CenterCrop((height_for_validation_cropping, \n",
    "                                                                 width_for_validation_cropping)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DocDataset(image_dir, label_dir, train_file_name,\n",
    "                           tf_sync_train, tf_image_train, tf_label_train)\n",
    "validation_dataset = DocDataset(image_dir, label_dir, validation_file_name,\n",
    "                                None, tf_image_validation, tf_label_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize_train, shuffle=True)\n",
    "# In validation, I'll save estimated label, therefore shuffle=True to save result for different input\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize_validation, shuffle=True)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('transform sync(image and label) for train : {}\\n'.format(tf_sync_train))\n",
    "    f.write('transform image for train : {}\\n'.format(tf_image_train))\n",
    "    f.write('transform label for train : {}\\n'.format(tf_label_train))\n",
    "    f.write('transform image for validation : {}\\n'.format(tf_image_validation))\n",
    "    f.write('transform label for validation : {}\\n\\n'.format(tf_label_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization\n",
    "# define parts for U-net for convenience (for encoder parts)\n",
    "# downsampling to half size (default)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:explore other normalization (because batch size is very small)\n",
    "# define parts for U-net for convenience (for decorder)\n",
    "# upsampling to double size (default) (using transposed convolution)\n",
    "# conv > batchnorm(optional) > dropout(optional) > relu\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize=4, stride=2, padding=1, use_bn=True, drop_prob=0.0):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.use_batchnorm = use_bn\n",
    "        self.use_dropout = drop_prob > 0\n",
    "        \n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=padding)\n",
    "        if self.use_batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if self.use_dropout:\n",
    "            self.dr = nn.Dropout(drop_prob)\n",
    "        self.rl = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        if self.use_batchnorm:\n",
    "            out = self.bn(out)\n",
    "        if self.use_dropout:\n",
    "            out = self.dr(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add attribute for switching using dropout or not and batchnorm or not\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, n_depth_encoder, n_base_channels=32):\n",
    "        super(U_Net, self).__init__()\n",
    "        \n",
    "        self.n_depth_encoder = n_depth_encoder\n",
    "        n_channels = 3\n",
    "        max_channels = 1024\n",
    "        \n",
    "        # encoder parts\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_base_channels, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_base_channels\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels*2, max_channels)\n",
    "                self.encoder.append(DownSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels*2\n",
    "                \n",
    "        # decoder parts\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                n_in_channels = min(n_channels, max_channels)\n",
    "                n_out_channels = min(n_channels, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "            else:\n",
    "                n_in_channels = min(n_channels, max_channels) + min(n_channels//2, max_channels)\n",
    "                n_out_channels = min(n_channels//2, max_channels)\n",
    "                self.decoder.append(UpSample(n_in_channels, n_out_channels))\n",
    "                n_channels = n_channels//2\n",
    "\n",
    "        # 1x1 convolution to adjust channels and refine result\n",
    "        self.conv1x1 = nn.Conv2d(n_channels, 3, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_encoders = []\n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i != 0:\n",
    "                out_encoders.append(x)\n",
    "            x = self.encoder[i](x)\n",
    "            \n",
    "        for i in range(self.n_depth_encoder):\n",
    "            if i == 0:\n",
    "                x = self.decoder[i](x)\n",
    "            else:\n",
    "                concated_input = torch.cat([x, out_encoders[self.n_depth_encoder-i-1]], dim=1)\n",
    "                x = self.decoder[i](concated_input)\n",
    "        \n",
    "        out = self.conv1x1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 94982979\n",
      "Model:\n",
      " U_Net(\n",
      "  (encoder): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (cv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): DownSample(\n",
      "      (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): DownSample(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): DownSample(\n",
      "      (cv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): DownSample(\n",
      "      (cv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): DownSample(\n",
      "      (cv): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (tc): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (1): UpSample(\n",
      "      (tc): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (2): UpSample(\n",
      "      (tc): ConvTranspose2d(1536, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (3): UpSample(\n",
      "      (tc): ConvTranspose2d(768, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (4): UpSample(\n",
      "      (tc): ConvTranspose2d(384, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "    (5): UpSample(\n",
      "      (tc): ConvTranspose2d(192, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      ")\n",
      "Loss:\n",
      " BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "net = U_Net(n_depth_encoder=6, n_base_channels=64)\n",
    "net = net.to(device)\n",
    "\n",
    "#TODO:explore good initialization\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# count the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# print settings\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('Model:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)\n",
    "print('Loss:\\n', criterion)\n",
    "\n",
    "with open(experiment_condition_txt, 'a') as f:\n",
    "    f.write('The number of trainable parameters : {}\\n'.format(num_trainable_params))\n",
    "    f.write('Model : \\n{}\\n\\n'.format(net))\n",
    "    f.write('Optimizer : \\n{}\\n\\n'.format(optimizer))\n",
    "    f.write('Loss : {}\\n\\n'.format(criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data_loader, epoch):\n",
    "    net.eval()\n",
    "    interval_save_images_epoch = 5\n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "        average_loss = running_loss / len(data_loader)\n",
    "        \n",
    "    # TODO:save experiment condition with result image name or separated config.json\n",
    "    # save image like (input, output, label) style for comparison\n",
    "    # use final minibatch\n",
    "    if epoch % interval_save_images_epoch == 0:\n",
    "        for i in range(batchsize_validation):\n",
    "            # unsqueeze to concat\n",
    "            input_image = inputs[i].unsqueeze(0)\n",
    "            \n",
    "            # expands to 3 channels to concat with input image\n",
    "            output_image = outputs[i].expand(3, *outputs[i].size()[1:]).unsqueeze(0)\n",
    "            label_image = labels[i].expand(3, *labels[i].size()[1:]).unsqueeze(0)\n",
    "            \n",
    "            # save image internally use make_grid and convert image like [3, 3, height, width] -> [3, height, width*3]\n",
    "            comparison_image = torch.cat([input_image, output_image, label_image])\n",
    "            save_image(comparison_image.data.cpu(), '{}input_output_GT_{}_{}.png'.format(result_image_dir, epoch, i))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[ 1/100] train_loss:0.6499 validation_loss:0.6636\n",
      "epoch[ 2/100] train_loss:0.6068 validation_loss:0.6414\n",
      "epoch[ 3/100] train_loss:0.5824 validation_loss:0.6067\n",
      "epoch[ 4/100] train_loss:0.5620 validation_loss:0.5672\n",
      "epoch[ 5/100] train_loss:0.5449 validation_loss:0.5303\n",
      "epoch[ 6/100] train_loss:0.5286 validation_loss:0.5074\n",
      "epoch[ 7/100] train_loss:0.5145 validation_loss:0.4916\n",
      "epoch[ 8/100] train_loss:0.5013 validation_loss:0.4775\n",
      "epoch[ 9/100] train_loss:0.4873 validation_loss:0.4585\n",
      "epoch[10/100] train_loss:0.4730 validation_loss:0.4380\n",
      "epoch[11/100] train_loss:0.4591 validation_loss:0.4272\n",
      "epoch[12/100] train_loss:0.4461 validation_loss:0.4109\n",
      "epoch[13/100] train_loss:0.4314 validation_loss:0.3888\n",
      "epoch[14/100] train_loss:0.4184 validation_loss:0.3835\n",
      "epoch[15/100] train_loss:0.4046 validation_loss:0.3667\n",
      "epoch[16/100] train_loss:0.3916 validation_loss:0.3656\n",
      "epoch[17/100] train_loss:0.3781 validation_loss:0.3546\n",
      "epoch[18/100] train_loss:0.3641 validation_loss:0.3488\n",
      "epoch[19/100] train_loss:0.3519 validation_loss:0.3298\n",
      "epoch[20/100] train_loss:0.3400 validation_loss:0.3266\n",
      "epoch[21/100] train_loss:0.3278 validation_loss:0.3173\n",
      "epoch[22/100] train_loss:0.3199 validation_loss:0.3347\n",
      "epoch[23/100] train_loss:0.3085 validation_loss:0.3012\n",
      "epoch[24/100] train_loss:0.2972 validation_loss:0.2911\n",
      "epoch[25/100] train_loss:0.2873 validation_loss:0.2732\n",
      "epoch[26/100] train_loss:0.2756 validation_loss:0.2681\n",
      "epoch[27/100] train_loss:0.2685 validation_loss:0.2668\n",
      "epoch[28/100] train_loss:0.2600 validation_loss:0.2532\n",
      "epoch[29/100] train_loss:0.2494 validation_loss:0.2344\n",
      "epoch[30/100] train_loss:0.2412 validation_loss:0.2366\n",
      "epoch[31/100] train_loss:0.2349 validation_loss:0.2373\n",
      "epoch[32/100] train_loss:0.2294 validation_loss:0.2149\n",
      "epoch[33/100] train_loss:0.2203 validation_loss:0.2034\n",
      "epoch[34/100] train_loss:0.2153 validation_loss:0.2003\n",
      "epoch[35/100] train_loss:0.2074 validation_loss:0.2128\n",
      "epoch[36/100] train_loss:0.1984 validation_loss:0.1846\n",
      "epoch[37/100] train_loss:0.1935 validation_loss:0.1828\n",
      "epoch[38/100] train_loss:0.1918 validation_loss:0.1870\n",
      "epoch[39/100] train_loss:0.1861 validation_loss:0.1822\n",
      "epoch[40/100] train_loss:0.1785 validation_loss:0.1698\n",
      "epoch[41/100] train_loss:0.1734 validation_loss:0.1627\n",
      "epoch[42/100] train_loss:0.1704 validation_loss:0.1725\n",
      "epoch[43/100] train_loss:0.1659 validation_loss:0.1548\n",
      "epoch[44/100] train_loss:0.1602 validation_loss:0.2217\n",
      "epoch[45/100] train_loss:0.1537 validation_loss:0.1561\n",
      "epoch[46/100] train_loss:0.1522 validation_loss:0.1529\n",
      "epoch[47/100] train_loss:0.1471 validation_loss:0.1436\n",
      "epoch[48/100] train_loss:0.1451 validation_loss:0.1373\n",
      "epoch[49/100] train_loss:0.1439 validation_loss:0.1370\n",
      "epoch[50/100] train_loss:0.1404 validation_loss:0.1334\n",
      "epoch[51/100] train_loss:0.1319 validation_loss:0.1272\n",
      "epoch[52/100] train_loss:0.1342 validation_loss:0.1220\n",
      "epoch[53/100] train_loss:0.1275 validation_loss:0.1556\n",
      "epoch[54/100] train_loss:0.1268 validation_loss:0.1825\n",
      "epoch[55/100] train_loss:0.1284 validation_loss:0.1658\n",
      "epoch[56/100] train_loss:0.1246 validation_loss:0.1189\n",
      "epoch[57/100] train_loss:0.1233 validation_loss:0.1150\n",
      "epoch[58/100] train_loss:0.1185 validation_loss:0.1060\n",
      "epoch[59/100] train_loss:0.1161 validation_loss:0.1189\n",
      "epoch[60/100] train_loss:0.1110 validation_loss:0.1358\n",
      "epoch[61/100] train_loss:0.1171 validation_loss:0.1037\n",
      "epoch[62/100] train_loss:0.1084 validation_loss:0.1008\n",
      "epoch[63/100] train_loss:0.1102 validation_loss:0.0995\n",
      "epoch[64/100] train_loss:0.1089 validation_loss:0.0953\n",
      "epoch[65/100] train_loss:0.1071 validation_loss:0.0970\n",
      "epoch[66/100] train_loss:0.1049 validation_loss:0.0918\n",
      "epoch[67/100] train_loss:0.1015 validation_loss:0.0908\n",
      "epoch[68/100] train_loss:0.0984 validation_loss:0.0936\n",
      "epoch[69/100] train_loss:0.1014 validation_loss:0.1091\n",
      "epoch[70/100] train_loss:0.1023 validation_loss:0.0877\n",
      "epoch[71/100] train_loss:0.0999 validation_loss:0.0924\n",
      "epoch[72/100] train_loss:0.0986 validation_loss:0.0883\n",
      "epoch[73/100] train_loss:0.0981 validation_loss:0.0981\n",
      "epoch[74/100] train_loss:0.0932 validation_loss:0.0862\n",
      "epoch[75/100] train_loss:0.0931 validation_loss:0.0808\n",
      "epoch[76/100] train_loss:0.0913 validation_loss:0.0785\n",
      "epoch[77/100] train_loss:0.0836 validation_loss:0.0778\n",
      "epoch[78/100] train_loss:0.0862 validation_loss:0.0849\n",
      "epoch[79/100] train_loss:0.0878 validation_loss:0.0766\n",
      "epoch[80/100] train_loss:0.0851 validation_loss:0.1015\n",
      "epoch[81/100] train_loss:0.0863 validation_loss:0.1139\n",
      "epoch[82/100] train_loss:0.0856 validation_loss:0.0798\n",
      "epoch[83/100] train_loss:0.0811 validation_loss:0.0874\n",
      "epoch[84/100] train_loss:0.0810 validation_loss:0.0710\n",
      "epoch[85/100] train_loss:0.0811 validation_loss:0.0799\n",
      "epoch[86/100] train_loss:0.0782 validation_loss:0.0827\n",
      "epoch[87/100] train_loss:0.0818 validation_loss:0.2177\n",
      "epoch[88/100] train_loss:0.0791 validation_loss:0.2062\n",
      "epoch[89/100] train_loss:0.0822 validation_loss:0.0662\n",
      "epoch[90/100] train_loss:0.0781 validation_loss:0.0692\n",
      "epoch[91/100] train_loss:0.0755 validation_loss:0.0661\n",
      "epoch[92/100] train_loss:0.0756 validation_loss:0.0635\n",
      "epoch[93/100] train_loss:0.0744 validation_loss:0.0936\n",
      "epoch[94/100] train_loss:0.0734 validation_loss:0.1031\n",
      "epoch[95/100] train_loss:0.0732 validation_loss:0.0696\n",
      "epoch[96/100] train_loss:0.0720 validation_loss:0.0652\n",
      "epoch[97/100] train_loss:0.0770 validation_loss:0.0654\n",
      "epoch[98/100] train_loss:0.0718 validation_loss:0.0759\n",
      "epoch[99/100] train_loss:0.0721 validation_loss:0.0627\n",
      "epoch[100/100] train_loss:0.0718 validation_loss:0.0611\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader, epoch)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%2d/%2d] train_loss:%1.4f validation_loss:%1.4f' % (epoch+1, n_epochs, train_loss, validation_loss))\n",
    "\n",
    "# TODO:save experiment condition with weight and log filename or separated config.json\n",
    "# save state dicts\n",
    "torch.save(net.state_dict(), log_dir + 'weight_' + str(epoch+1) + '.pth')\n",
    "\n",
    "# save learning log\n",
    "np.save(log_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(log_dir + 'validation_loss_list.npy', np.array(validation_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8b444dbe0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(validation_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ64, 学習率5e-4というのが1つのうまく行く組合せらしい（他にもあると思うが、まだ見つけられていない）\n",
    "# 過学習を起こしている様子はあまりないが、validation lossの安定感に非常に欠ける\n",
    "# フィルタサイズは大きい方が性能が良い（当然といえば当然） 16とかだとマジで結果が悪い\n",
    "# 学習率のグラフで結果がたまに跳ね上がることがあるが、それは恐らく絵の画像に突き当たったりしているため"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
